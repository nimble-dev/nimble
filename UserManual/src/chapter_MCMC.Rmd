# (PART) Algorithms in NIMBLE {-}

```{r, echo=FALSE}
require(nimble)
``` 

# MCMC {#cha:mcmc}

```{r, MCMCchunk0, echo = FALSE}
# source the code
if(!require(nimble, warn.conflicts = FALSE, quietly = TRUE)) {
#source(file.path('..', '..', 'examples', 'demos', 'loadAllCode.R'))
}
require(methods, warn.conflicts = FALSE, quietly = TRUE)  # seems to be needed, but why?
require(igraph, warn.conflicts = FALSE, quietly = TRUE)  # same question
``` 

NIMBLE provides a variety of paths to creating and executing an MCMC algorithm, which differ greatly in their simplicity of use, and also in the options available and customizability.

The most direct approach to invoking the MCMC engine is using the `nimbleMCMC` function (Section \@ref(sec:nimbleMCMC)).  This one-line call creates and executes an MCMC, and provides a wide range of options for controlling the MCMC: specifying monitors, burn-in, and thinning, running multiple MCMC chains with different initial values, and returning posterior samples, summary statistics, and/or a WAIC value.  However, this approach is restricted to using NIMBLE's default MCMC algorithm; further customization of, for example, the specific samplers employed, is not possible.

The lengthier and more customizable approach to invoking the MCMC engine on a particular NIMBLE model object involves the following steps:


  1. (Optional) Create and customize an MCMC configuration for a particular model:
  
      a. Use `configureMCMC` to create an MCMC configuration (see Section \@ref(sec:mcmc-configuration)).  The configuration contains a list of samplers with the node(s) they will sample.
      a. (Optional) Customize the MCMC configuration: 
    
          i. Add, remove, or re-order the list of samplers (Section \@ref(sec:samplers-provided) and `help(samplers)` in R for details), including adding your own samplers (Section \@ref(sec:user-samplers));
          i. Change the tuning parameters or adaptive properties of individual samplers;
          i. Change the variables to monitor (record for output) and thinning intervals for MCMC samples.
  1. Use `buildMCMC` to build the MCMC object and its samplers either from the model (using default MCMC configuration) or from a customized MCMC configuration (Section \@ref(sec:build-compile-mcmc)).
  1. Compile the MCMC object (and the model), unless one is debugging and wishes to run the uncompiled MCMC.
  1. Run the MCMC and extract the samples (Sections \@ref(sec:runMCMC), \@ref(sec:executing-the-mcmc-algorithm) and \@ref(sec:extracting-samples)).
  1. Optionally, calculate the WAIC (Section \@ref(sec:WAIC)).

Prior to version 0.8.0, NIMBLE provided two additional functions, `MCMCsuite` and `compareMCMCs`, to facilitate comparison of multiple MCMC algorithms, either internal or external to NIMBLE.  Those capabilities have been redesigned and moved into a separate package called `compareMCMCs`.

End-to-end examples of MCMC in NIMBLE can be found in Sections \@ref(sec:creating-mcmc)-\@ref(sec:customizing-mcmc) and Section \@ref(sec:mcmc-example-litters).
<!---  ### Creating an MCMC algorithm -->

<!---  A default MCMC algorithm can be created using `buildMCMC(model)`.  See Section \@ref(sec:build-compile-mcmc). -->

<!---  To customize properties of the MCMC algorithm -- including the sampling algorithms, joint (block) sampling of dimensions, tuning and adaptive properties of sampling algorithms, variables being monitored (posterior samples are collected), and the thinning interval for sample collection -- an MCMC configuration object (`conf`) must be created using `configureMCMC(model)`.  Properties of the MCMC algorithm can be modified using the `conf` object, after which an MCMC algorithm (`mcmc`) can be created using `buildMCMC(conf)`.  See Section \@ref(mcmc-configuration). -->

<!---  Both the `model` and `mcmc` should generally be compiled using `compileNimble`, for significantly faster execution (Section \@ref(sec:build-compile-mcmc)).  We'll refer to the compiled MCMC algorithm as `Cmcmc`. -->

<!---  ### Executing an MCMC algorithm and collecting samples -->

<!---  There are two methods available to execute an MCMC algorithm: -->

<!---   
%   1. `Cmcmc$run(...)`
%    1. `runMCMC(Cmcmc, ...)` -->
<!---  Using `Cmcmc$run(...)` provides lower-level options including extending an MCMC run (collecting additional samples from where it last stopped), and also collecting timing information for the internal sampling algorithms.  After executing an MCMC in this manner, posterior samples are extracted from within the `Cmcmc` object using `as.matrix(Cmcmc$mvSamples)`.  See Sections \@ref(sec:executing-the-mcmc-algorithm) and \@ref(sec:extracting-samples). -->

<!---  Using `runMCMC(Cmcmc, ...)` provides higher-level options such as running multiple chains, setting initial values, removing burn-in, and returning posterior samples in the form of a `coda` `mcmc` object.  Running an MCMC in this manner returns an array of samples, a list of sample arrays in the case of multiple chains, or optionally a `coda` object.  See Section \@ref(sec:runMCMC). -->

<!---  ### Other topics -->

<!---  In addition to details of the steps outlined above, this chapter also includes: -->

<!---   
% %%  1. NIMBLE's algorithm to search blocks of nodes for efficient joint (block) sampling (section \@ref(sec:default-mcmc-conf)) -->
<!---    1. Information about the sampling algorithms provided with NIMBLE (Section \@ref(sec:samplers-provided)) 
%   1. A detailed example of using the MCMC engine (section \@ref(sec:mcmc-example-litters)) --> 
<!---    1. Methods to automatically run WinBUGS, OpenBUGS, JAGS, Stan and/or multiple NIMBLE MCMCs on the same model (section \@ref(mcmc-suite-compare-mcmcs))  -->


## One-line invocation of MCMC: *nimbleMCMC* {#sec:nimbleMCMC}

The most direct approach to executing an MCMC algorithm in NIMBLE is using `nimbleMCMC`.  This single function can be used to create an underlying model and associated MCMC algorithm, compile both of these, execute the MCMC, and return samples, summary statistics, and a WAIC value.  This approach circumvents the longer (and more flexible) approach using `nimbleModel`, `configureMCMC`, `buildMCMC`, `compileNimble`, and `runMCMC`, which is described subsequently.

The `nimbleMCMC` function provides control over the:


  - number of MCMC iterations in each chain;
  - number of MCMC chains to execute;
  - number of burn-in samples to discard from each chain;
  - thinning interval on which samples should be recorded;
  - model variables to monitor and return posterior samples;
  - initial values, or a function for generating initial values for each chain;
  - setting the random number seed;
  - returning posterior samples as a matrix or a `coda` `mcmc` object;
  - returning posterior summary statistics; and
  - returning a WAIC value calculated using post-burn-in samples from all chains.


This entry point for using `nimbleMCMC` is the `code`, `constants`, `data`, and `inits` arguments that are used for building a NIMBLE model (see Chapters \@ref(cha:writing-models) and \@ref(cha:building-models)).  However, when using `nimbleMCMC`, the `inits` argument can also specify a list of lists of initial values that will be used for each MCMC chain, or a function that generates a list of initial values, which will be generated at the onset of each chain.  As an alternative entry point, a NIMBLE `model` object can also be supplied to `nimbleMCMC`, in which case this model will be used to build the MCMC algorithm.

Based on its arguments, `nimbleMCMC` optionally returns any combination of


  - Posterior samples,
  - Posterior summary statistics, and
  - WAIC value.


The above are calculated and returned for each MCMC chain, using the post-burn-in and thinned samples.  Additionally, posterior summary statistics are calculated for all chains combined when multiple chains are run.

Several example usages of `nimbleMCMC` are shown below:

```{r, nimbleMCMC, eval=FALSE}
code <- nimbleCode({
    mu ~ dnorm(0, sd = 1000)
    sigma ~ dunif(0, 1000)
    for(i in 1:10)
        x[i] ~ dnorm(mu, sd = sigma)
})
data <- list(x = c(2, 5, 3, 4, 1, 0, 1, 3, 5, 3))
initsFunction <- function() list(mu = rnorm(1,0,1), sigma = runif(1,0,10))

# execute one MCMC chain, monitoring the "mu" and "sigma" variables,
# with thinning interval 10.  fix the random number seed for reproducible
# results.  by default, only returns posterior samples.
mcmc.out <- nimbleMCMC(code = code, data = data, inits = initsFunction,
                       monitors = c("mu", "sigma"), thin = 10,
                       niter = 20000, nchains = 1, setSeed = TRUE)

# note that the inits argument to nimbleModel must be a list of
# initial values, whereas nimbleMCMC can accept inits as a function
# for generating new initial values for each chain.
initsList <- initsFunction()
Rmodel <- nimbleModel(code, data = data, inits = initsList)

# using the existing Rmodel object, execute three MCMC chains with 
# specified burn-in.  return samples, summary statistics, and WAIC.
mcmc.out <- nimbleMCMC(model = Rmodel,
                       niter = 20000, nchains = 3, nburnin = 2000,
                       summary = TRUE, WAIC = TRUE)

# run ten chains, generating random initial values for each
# chain using the inits function specified above.
# only return summary statistics from each chain; not all the samples.
mcmc.out <- nimbleMCMC(model = Rmodel, nchains = 10, inits = initsFunction,
                       samples = FALSE, summary = TRUE)


```

See `help(nimbleMCMC)` for further details.





## The MCMC configuration {#sec:mcmc-configuration}

The MCMC configuration contains information needed for building an MCMC.  When no customization is needed, one can jump directly to the `buildMCMC` step below.  An MCMC configuration is an object of class `MCMCconf`, which includes:


  - The model on which the MCMC will operate
  - The model nodes which will be sampled (updated) by the MCMC
  - The samplers and their internal configurations, called control parameters
  - Two sets of variables that will be monitored (recorded) during execution of the MCMC and thinning intervals for how often each set will be recorded. Two sets are allowed because it can be useful to monitor different variables at different intervals


### Default MCMC configuration {#sec:default-mcmc-conf}

Assuming we have a model named `Rmodel`, the following will generate a default MCMC configuration:
```{r, mcmcConf, eval=FALSE}
mcmcConf <- configureMCMC(Rmodel)
```

The default configuration will contain a single sampler for each node in the model, and the default ordering follows the topological ordering of the model.

#### Default assignment of sampler algorithms

The default sampler assigned to a stochastic node is determined by the following, in order of precedence:


  1. If the node has no stochastic dependents, a `posterior_predictive` sampler is assigned.  This sampler sets the new value for the node simply by simulating from its distribution.
  1. If the node has a conjugate relationship between its prior distribution and the distributions of its stochastic dependents, a `conjugate` (`Gibbs') sampler is assigned.
  1. If the node follows a multinomial distribution, then a `RW_multinomial` sampler is assigned.  This is a discrete random-walk sampler in the space of multinomial outcomes.
  1. If a node follows a Dirichlet distribution, then a `RW_dirichlet` sampler is assigned.  This is a random walk sampler in the space of the simplex defined by the Dirichlet.
  1. If a node follows an LKJ correlation distribution, then a `RW_block_lkj_corr_cholesky` sampler is assigned. This is a block random walk sampler in a transformed space where the transformation uses the signed stickbreaking approach described in Section 10.12 of [@Stan_2021].
  1. If the node follows any other multivariate distribution, then a `RW_block` sampler is assigned for all elements.  This is a Metropolis-Hastings adaptive random-walk sampler with a multivariate normal proposal [@Roberts_Sahu_1997].
  1. If the node is binary-valued (strictly taking values 0 or 1), then a `binary` sampler is assigned.  This sampler calculates the conditional probability for both possible node values and draws the new node value from the conditional distribution, in effect making a Gibbs sampler.
  1. If the node is otherwise discrete-valued, then a `slice` sampler is assigned [@Neal2003].
  1. If none of the above criteria are satisfied, then a `RW` sampler is assigned.  This is a Metropolis-Hastings adaptive random-walk sampler with a univariate normal proposal distribution.


<!-- These sampler assignment rules can be inspected, reordered, and easily modified using the system option `nimbleOptions("MCMCdefaultSamplerAssignmentRules")` and customized  `samplerAssignmentRules` objects. -->

Details of each sampler and its control parameters can be found by invoking `help(samplers)`.

<!-- #### Sampler assignment rules

The behavior of `configureMCMC` can be customized to control how samplers are assigned.  A new set of sampler assignment rules can be created using `samplerAssignmentRules`, which can be modified using the `addRule` and `reorder` methods, then passed as an argument to `configureMCMC`.  Alternatively, the default behavior of `configureMCMC` can be altered by setting the system option `MCMCdefaultSamplerAssignmentRules` to a custom `samplerAssignmentRules` object.  See `help(samplerAssignmentRules)` for details. -->

#### Options to control default sampler assignments

Very basic control of default sampler assignments is provided via two arguments to `configureMCMC`.  The `useConjugacy` argument controls whether conjugate samplers are assigned when possible, and the  `multivariateNodesAsScalars` argument controls whether scalar elements of multivariate nodes are sampled individually. See `help(configureMCMC)` for usage details.
<!--- % The following optional control arguments to `configureMCMC()` may be used to override the default assignment of sampler algorithms: -->

<!--- 
%%   1.[useConjugacy (default `TRUE`)] If `TRUE`, conjugate samplers will be assigned to nodes determined to be in conjugate relationships.  If `FALSE`, no conjugate samplers will be assigned.
%   1.[multivariateNodesAsScalars (default `FALSE`)]  If `TRUE`, then independent scalar random walk Metropolis-Hastings samplers (`RW`) will be assigned to all scalar components comprising multivariate nodes.  This contrasts the default behavior of a single block sampler being assigned to multivariate nodes.  Regardless of the value of this argument, conjugate samplers will be assigned to conjugate (scalar and multivariate nodes), provided `useConjugacy = TRUE`. -->
 

#### Default monitors

The default MCMC configuration includes monitors on all top-level stochastic nodes of the model. Only variables that are monitored will have their samples saved for use outside of the MCMC. MCMC configurations include two sets of monitors, each with different thinning intervals.  By default, the second set of monitors (`monitors2`) is empty. 

#### Automated parameter blocking

\emph{The automated parameter blocking algorithm is no longer actively maintained.  In some cases, it may not operate correctly with more recent system features and/or distributions.}

The default configuration may be replaced by one generated from an automated parameter blocking algorithm.  This algorithm determines groupings of model nodes that, when jointly sampled with a `RW_block` sampler, increase overall MCMC efficiency.  Overall efficiency is defined as the effective sample size of the slowest-mixing node divided by computation time.  This is done by:

```{r, mcmcconf5, eval=FALSE}
autoBlockConf <- configureMCMC(Rmodel, autoBlock = TRUE)
```

Note that this using `autoBlock = TRUE` compiles and runs MCMCs, progressively exploring different sampler assignments, so it takes some time and generates some output.  It is most useful for determining effective blocking strategies that can be re-used for later runs.  The additional control argument `autoIt` may also be provided to indicate the number of MCMC samples to be used in each trial of the automated blocking procedure (default 20,000). 

### Customizing the MCMC configuration {#sec:customizing-mcmc-conf}

The MCMC configuration may be customized in a variety of ways, either through additional named arguments to `configureMCMC` or by calling methods of an existing `MCMCconf` object.

#### Controlling which nodes to sample

One can create an MCMC configuration with default samplers on just a particular set of nodes using the `nodes` argument to `configureMCMC`. The value for the `nodes` argument may be a character vector containing node and/or variable names.  In the case of a variable name, a default sampler will be added for all stochastic nodes in the variable.  The order of samplers will match the order of `nodes`.  Any deterministic nodes will be ignored.

If a data node is included in `nodes`, *it will be assigned a sampler*.  This is the only way in which a default sampler may be placed on a data node and will result in overwriting data values in the node.

#### Creating an empty configuration 
If you plan to customize the choice of all samplers, it can be useful to obtain a configuration with no sampler assignments at all.  This can be done by any of `nodes = NULL`, `nodes = character()`, or `nodes = list()`. 

<!-- #### Overriding the default sampler assignment rules
The default rules used for assigning samplers to model nodes can be overridden using the `rules` argument to `configureMCMC`.  This argument must be an object of class `samplerAssignmentRules`, which defines an ordered set of rules for assigning samplers.  Rules can be modified and reordered, to give different precedence to particular samplers, or to assign user-defined samplers (see section \@ref(sec:user-samplers)).  The following example creates a new set of rules (which initially contains the default assignment rules), reorders the rules, adds a new rule, then uses these rules to create an MCMC configuration object.

r, mcmcconfrules, eval=FALSE}
my_rules <- samplerAssignmentRules()
my_rules$reorder(c(8, 1:7))
my_rules$addRule(condition = quote(model$getDistribution(node) == "dmnorm"),
                 sampler = new_dmnorm_sampler)
mcmcConf <- configureMCMC(Rmodel, rules = my_rules)

In addition, the default behavior of `configureMCMC` can be altered by setting the system option `nimbleOptions(MCMCdefaultSamplerAssignmentRules = my_rules)`, or reset to the original default behavior using `nimbleOptions(MCMCdefaultSamplerAssignmentRules = samplerAssignmentRules())`. -->

#### Overriding the default sampler control list values
The default values of control list elements for all sampling algorithms may be overridden through use of the `control` argument to `configureMCMC`, which should be a named list. 
Named elements in the `control` argument will be used for all default samplers and any subsequent sampler added via `addSampler` (see below).  For example, the following will create the default MCMC configuration, except all `RW` samplers will have their initial `scale` set to 3, and none of the samplers (`RW`, or otherwise) will be adaptive.

```{r, mcmcconf6, eval=FALSE}
mcmcConf <- configureMCMC(Rmodel, control = list(scale = 3, adaptive = FALSE))
```

When adding samplers to a configuration using `addSampler`, the default control list can also be over-ridden.

#### Adding samplers to the configuration: *addSampler*

Samplers may be added to a configuration using the `addSampler` method of the `MCMCconf` object.  The first argument gives the node(s) to be sampled, called the `target`, as a character vector.   The second argument gives the type of sampler, which may be provided as a character string or a nimbleFunction object. Valid character strings are indicated in `help(samplers)` (do not include `"sampler_"`).  Added samplers can be labeled with a `name` argument, which is used in output of `printSamplers`.

Writing a new sampler as a nimbleFunction is covered in Section \@ref(sec:user-samplers).

The hierarchy of precedence for control list elements for samplers is:


  1. The `control` list argument to `addSampler`;
  1. The `control` list argument to `configureMCMC`;
  1. The default values, as defined in the sampling algorithm `setup` function.


Samplers added by `addSampler` will be appended to the end of current sampler list.  Adding a sampler for a node will *not* automatically remove any existing samplers on that node. 

#### Printing, re-ordering, modifying and removing samplers: *printSamplers*, *removeSamplers*, *setSamplers*, and *getSamplerDefinition*

The current, ordered, list of all samplers in the MCMC configuration may be printed by calling the `printSamplers` method. When you want to see only samplers acting on specific model nodes or variables, provide those names as an argument to `printSamplers`.  The `printSamplers` method accepts arguments controlling the level of detail displayed as discussed in its R help information.

```{r, printSamplers, eval=FALSE}
# Print all samplers
mcmcConf$printSamplers()

# Print all samplers operating on node "a[1]",
# or any of the "beta[]" variables
mcmcConf$printSamplers(c("a[1]", "beta"))

# Print all conjugate and slice samplers
mcmcConf$printSamplers(type = c("conjugate", "slice"))

# Print all RW samplers operating on "x"
mcmcConf$printSamplers("x", type = "RW")

# Print the first 100 samplers
mcmcConf$printSamplers(1:100)

# Print all samplers in their order of execution
mcmcConf$printSamplers(executionOrder = TRUE)
```

Samplers may be removed from the configuration object using `removeSamplers`, which accepts a character vector of node or variable names, or a numeric vector of indices.

```{r, removeSamplers, eval=FALSE}
# Remove all samplers acting on "x" or any component of it
mcmcConf$removeSamplers("x")

# Remove all samplers acting on "alpha[1]" and "beta[1]"
mcmcConf$removeSamplers(c("alpha[1]", "beta[1]"))

# Remove the first five samplers
mcmcConf$removeSamplers(1:5)

# Providing no argument removes all samplers
mcmcConf$removeSamplers()
```

Samplers to retain may be specified reordered using `setSamplers`, which also accepts a character vector of node or variable names, or a numeric vector of indices.

```{r, setSamplers2, eval=FALSE}
# Set the list of samplers to those acting on any components of the
# model variables "x", "y", or "z".
mcmcConf$setSamplers(c("x", "y", "z"))

# Set the list of samplers to only those acting on model nodes
# "alpha[1]", "alpha[2]", ..., "alpha[10]"
mcmcConf$setSamplers("alpha[1:10]")

# Truncate the current list of samplers to the first 10 and the 100th
mcmcConf$setSamplers(ind = c(1:10, 100))
```

The nimbleFunction definition underlying a particular sampler may be viewed using the `getSamplerDefinition` method, using the sampler index as an argument.  A node name argument may also be supplied, in which case the definition of the first sampler acting on that node is returned.  In all cases, `getSamplerDefinition` only returns the definition of the *first* sampler specified either by index or node name.

```{r, getSamplerDefinition, eval=FALSE}
# Return the definition of the third sampler in the mcmcConf object
mcmcConf$getSamplerDefinition(3)

# Return the definition of the first sampler acting on node "x",
# or the first of any indexed nodes comprising the variable "x"
mcmcConf$getSamplerDefinition("x")
```

#### Customizing individual sampler configurations: *getSamplers*, *setSamplers*, *setName*, *setSamplerFunction*, *setTarget*, and *setControl*

Each sampler in an `MCMCconf` object is represented by a sampler configuration as a `samplerConf` object.  Each `samplerConf` is a reference class object containing the following (required) fields: `name` (a character string), `samplerFunction` (a valid nimbleFunction sampler), `target` (the model node to be sampled), and `control` (list of control arguments).  The `MCMCconf` method `getSamplers` allows access to the `samplerConf` objects.  These can be modified and then passed as an argument to `setSamplers` to over-write the current list of samplers in the MCMC configuration object.  However, no checking of the validity of this modified list is performed; if the list of samplerConf objects is corrupted to be invalid, incorrect behavior will result at the time of calling `buildMCMC`.  The fields of a `samplerConf` object can be modified using the access functions `setName(name)`, `setSamplerFunction(fun)`, `setTarget(target, model)`, and `setControl(control)`.

Here are some examples:

```{r, getSetSamplers, eval=FALSE}
# retrieve samplerConf list
samplerConfList <- mcmcConf$getSamplers()

# change the name of the first sampler
samplerConfList[[1]]$setName("newNameForThisSampler")

# change the sampler function of the second sampler,
# assuming existance of a nimbleFunction 'anotherSamplerNF',
# which represents a valid nimbleFunction sampler.
samplerConfList[[2]]$setSamplerFunction(anotherSamplerNF)

# change the 'adaptive' element of the control list of the third sampler
control <- samplerConfList[[3]]$control
control$adaptive <- FALSE
samplerConfList[[3]]$setControl(control)

# change the target node of the fourth sampler
samplerConfList[[4]]$setTarget("y", model)   # model argument required

# use this modified list of samplerConf objects in the MCMC configuration
mcmcConf$setSamplers(samplerConfList)
```

#### Customizing the sampler execution order

The ordering of sampler execution can be controlled as well.  This allows for sampler functions to execute multiple times within a single MCMC iteration, or the execution of different sampler functions to be interleaved with one another.

The sampler execution order is set using the function `setSamplerExecutionOrder`, and the current ordering of execution is retrieved using `getSamplerExecutionOrder`.  For example, assuming the MCMC configuration object `mcmcConf` contains five samplers:

```{r, setSamplerExecutionOrder, eval=FALSE}
# first sampler to execute twice, in succession:
mcmcConf$setSamplerExecutionOrder(c(1, 1, 2, 3, 4, 5))

# first sampler to execute multiple times, interleaved:
mcmcConf$setSamplerExecutionOrder(c(1, 2, 1, 3, 1, 4, 1, 5))

# fourth sampler to execute 10 times, only
mcmcConf$setSamplerExecutionOrder(rep(4, 10))

# omitting the argument to setSamplerExecutionOrder()
# resets the ordering to each sampler executing once, sequentially
mcmcConf$setSamplerExecutionOrder()

# retrieve the current ordering of sampler execution
ordering <- mcmcConf$getSamplerExecutionOrder()

# print the sampler functions in the order of execution
mcmcConf$printSamplers(executionOrder = TRUE)
```

#### Monitors and thinning intervals: *printMonitors*, *getMonitors*, *setMonitors*, *addMonitors*, *resetMonitors* and *setThin*

An MCMC configuration object contains two independent sets of variables to monitor, each with their own thinning interval: `thin` corresponding to `monitors`, and `thin2` corresponding to `monitors2`.  Monitors operate at the *variable* level.  Only entire model variables may be monitored.  Specifying a monitor on a *node*, e.g., `x[1]`, will result in the entire variable `x` being monitored.

The variables specified in `monitors`  and `monitors2` will be recorded (with thinning interval `thin`) into objects called `mvSamples` and `mvSamples2`, contained within the MCMC object. These are both *modelValues* objects; modelValues are NIMBLE data structures used to store multiple sets of values of model variables^[See Section \@ref(sec:modelValues-struct) for general information on modelValues.].  These can be accessed as the member data `mvSamples` and `mvSamples2` of the MCMC object, and they can be converted to matrices using `as.matrix` or lists using `as.list` (see Section \@ref(sec:extracting-samples)).

Monitors may be added to the MCMC configuration either in the original call to `configureMCMC` or using the `addMonitors` method:
```{r, addMonitors, eval=FALSE}
# Using an argument to configureMCMC
mcmcConf <- configureMCMC(Rmodel, monitors = c("alpha", "beta"), 
                          monitors2 = "x")

# Calling a member method of the mcmcconf object
# This results in the same monitors as above
mcmcConf$addMonitors("alpha", "beta")
mcmcConf$addMonitors2("x")
```

A new set of monitor variables can be added to the MCMC configuration, overwriting the current monitors, using the `setMonitors` method:
```{r, setMonitors, eval=FALSE}
# Replace old monitors, now monitor "delta" and "gamma" only
mcmcConf$setMonitors("gamma", "delta")
```

Similarly, either thinning interval may be set at either step:
```{r, thinning, eval=FALSE}
# Using an argument to configureMCMC
mcmcConf <- configureMCMC(Rmodel, thin = 1, thin2 = 100)

# Calling a member method of the mcmcConf object
# This results in the same thinning intervals as above
mcmcConf$setThin(1)
mcmcConf$setThin2(100)
```

The current lists of monitors and thinning intervals may be displayed using the `printMonitors` method.  Both sets of monitors (`monitors` and `monitors2`) may be reset to empty character vectors by calling the `resetMonitors` method.  The methods `getMonitors`  and `getMonitors2` return the currently specified `monitors` and `monitors2` as character vectors.

#### Monitoring model log-probabilities

To record model log-probabilities from an MCMC, one can add monitors for *logProb* variables (which begin with the prefix `logProb_`) that correspond to variables with (any) stochastic nodes.   For example, to record and extract log-probabilities for the variables `alpha`, `sigma_mu`, and `Y`:
```{r, eval=FALSE}
mcmcConf <- configureMCMC(Rmodel, enableWAIC = TRUE)
mcmcConf$addMonitors("logProb_alpha", "logProb_sigma_mu", "logProb_Y")
Rmcmc <- buildMCMC(mcmcConf)
Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)
Cmcmc$run(10000)
samples <- as.matrix(Cmcmc$mvSamples)
``` 

The `samples` matrix will contain both MCMC samples and model log-probabilities.



## Building and compiling the MCMC {#sec:build-compile-mcmc}

Once the MCMC configuration object has been created, and customized to one's liking, it may be used to build an MCMC function:

```{r, buildMCMC, eval=FALSE}
Rmcmc <- buildMCMC(mcmcConf)
```

`buildMCMC` is a nimbleFunction.  The returned object `Rmcmc` is an instance  of the nimbleFunction specific to configuration `mcmcConf` (and of course its associated model).  

Note that if you would like to be able to calculate the WAIC of the model, you should usually set `enableWAIC = TRUE` as an argument to`configureMCMC` (or to `buildMCMC` if not using `configureMCMC`), or set `nimbleOptions(MCMCenableWAIC = TRUE)`, which will enable WAIC calculations for all subsequently built MCMC functions.  For more information on WAIC calculations, including situations in which you can calculate WAIC without having set `enableWAIC = TRUE` see Section \@ref(sec:WAIC) or `help(waic)` in R.

When no customization is needed, one can skip `configureMCMC` and simply provide a model object to `buildMCMC`. The following two MCMC functions will be identical:

```{r, overloadedBuildMCMC, eval=FALSE}
mcmcConf <- configureMCMC(Rmodel)   # default MCMC configuration
Rmcmc1 <- buildMCMC(mcmcConf)

Rmcmc2 <- buildMCMC(Rmodel)   # uses the default configuration for Rmodel
```

For speed of execution, we usually want to compile the MCMC function to C++ (as is the case for other NIMBLE functions).  To do so, we use `compileNimble`.  If the model has already been compiled, it should be provided as the `project` argument so the MCMC will be part of the same compiled project.  A typical compilation call looks like:

```{r, compileMCMC, eval=FALSE}
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)
```

Alternatively, if the model has not already been compiled, they can be compiled together in one line:

```{r, compileMCMC2, eval=FALSE}
Cmcmc <- compileNimble(Rmodel, Rmcmc)
```

Note that if you compile the MCMC with another object (the model in this case), you'll need to explicitly refer to the MCMC component of the resulting object to be able to run the MCMC:

```{r, runMCMC2, eval=FALSE}
Cmcmc$Rmcmc$run(niter = 1000)
```



## User-friendly execution of MCMC algorithms: *runMCMC* {#sec:runMCMC}

Once an MCMC algorithm has been created using `buildMCMC`, the function `runMCMC` can be used to run multiple chains and extract posterior samples, summary statistics and/or a WAIC value.  This is a simpler approach to executing an MCMC algorithm, than the process of executing and extracting samples as described in Sections \@ref(sec:executing-the-mcmc-algorithm) and \@ref(sec:extracting-samples).

`runMCMC` also provides several user-friendly options such as burn-in, thinning, running multiple chains, and different initial values for each chain.  However, using `runMCMC` does not support several lower-level options, such as timing the individual samplers internal to the MCMC, continuing an existing MCMC run (picking up where it left off), or modifying the sampler execution ordering.

`runMCMC` takes arguments that will control the following aspects of the MCMC:
<!---  `runMCMC` has one mandatory argument: the (compiled or uncompiled) MCMC algorithm.  Other arguments to `runMCMC` are available to control: -->


  - Number of iterations in each chain;
  - Number of chains;
  - Number of burn-in samples to discard from each chain;
  - Thinning interval for recording samples;
  - Initial values, or a function for generating initial values for each chain;
  - Setting the random number seed;
  - Returning the posterior samples as a `coda` `mcmc` object;
  - Returning summary statistics calculated from each chains; and
  - Returning a WAIC value calculated using (post-burn-in) samples from all chains.

<!---  See `help(runMCMC)` for details of these arguments. -->

<!---  Other features of the MCMC can still be customized when using `runMCMC`, which is done using the MCMC configuration object.  This includes setting monitors, or customizing samplers (see Section \@ref(sec:customizing-mcmc-conf)).  The MCMC configuration object is then used to create an MCMC algorithm, which is compiled and passed as an argument to `runMCMC`. -->

<!---  When running a single chain, `runMCMC` returns a matrix of MCMC samples (as described in Section \@ref(sec:extracting-samples)).  When running multiple chains, a list of sample matrices is returned.  Using the argument `samplesAsCodaMCMC=TRUE`, a `coda` `mcmc` object, or in the case of multiple chains, a `coda` `mcmc.list` object is returned instead. -->

The following examples demonstrate some uses of `runMCMC`, and assume the existence of `Cmcmc`, a compiled MCMC algorithm.

```{r, runMCMC, eval=FALSE}
# run a single chain, and return a matrix of samples
mcmc.out <- runMCMC(Cmcmc)

# run three chains of 10000 samples, discard initial burn-in of 1000,
# record samples thereafter using a thinning interval of 10,
# and return of list of sample matrices
mcmc.out <- runMCMC(Cmcmc, niter=10000, nburnin=1000, thin=10, nchains=3)

# run three chains, returning posterior samples, summary statistics,
# and the WAIC value
mcmc.out <- runMCMC(Cmcmc, nchains = 3, summary = TRUE, WAIC = TRUE)

# run two chains, and specify the initial values for each
initsList <- list(list(mu = 1, sigma = 1),
                  list(mu = 2, sigma = 10))
mcmc.out <- runMCMC(Cmcmc, nchains = 2, inits = initsList)

# run ten chains of 100,000 iterations each, using a function to 
# generate initial values and a fixed random number seed for each chain.
# only return summary statistics from each chain; not all the samples.
initsFunction <- function()
    list(mu = rnorm(1,0,1), sigma = runif(1,0,100))
mcmc.out <- runMCMC(Cmcmc, niter = 100000, nchains = 10,
                    inits = initsFunction, setSeed = TRUE,
                    samples = FALSE, summary = TRUE)
```


See `help(runMCMC)` for further details.


## Running the MCMC {#sec:executing-the-mcmc-algorithm}

The MCMC algorithm (either the compiled or uncompiled version) can be executed using the member method `mcmc$run` (see `help(buildMCMC)` in R).  The `run` method has one required argument, `niter`, the number of iterations to be run.

The `run` method has optional arguments `nburnin`, `thin` and `thin2`.  These can be used to specify the number of pre-thinning burn-in samples to discard, and the post-burnin thinning intervals for recording samples (corresponding to `monitors` and `monitors2`).  If either `thin` and `thin2` are provided, they will override the thinning intervals that were specified in the original MCMC configuration object.

### Rerunning versus restarting an MCMC {#sec:mcmc-rerun}

The `run` method has an optional `reset` argument.  When `reset = TRUE` (the default value), the following occurs prior to running the MCMC:

  1. All model nodes are checked and filled or updated as needed, in valid (topological) order.  If a stochastic node is missing a value, it is populated using a call to `simulate` and its log probability value is calculated.  The values of deterministic nodes are calculated from their parent nodes.  If any right-hand-side-only nodes (e.g., explanatory variables) are missing a value, an error results.
  1. All MCMC sampler functions are reset to their initial state: the initial values of any sampler control parameters (e.g., `scale`, `sliceWidth`, or `propCov`) are reset to their initial values, as were specified by the original MCMC configuration.
  1. The internal modelValues objects `mvSamples` and `mvSamples2` are each resized to the appropriate length for holding the requested number of samples (`niter/thin`, and `niter/thin2`, respectively).

This means that one can begin a new run of an existing MCMC without having to rebuild or recompile the model or the MCMC. This can be helpful if one wants to use the same model and MCMC configuration, but with different initial values, different values of data nodes (but which nodes are data nodes must be the same), changes to covariate values(or other non-data, non-parameter values) in the model, or a different number of MCMC iterations, thinning interval or burn-in.

In contrast, when `mcmc$run(niter, reset = FALSE)` is called, the MCMC picks up from where it left off, continuing the previous chain and expanding the output as needed.  No values in the model are checked or altered, and sampler functions are not reset to their initial states.

The `run` method also has an optional `resetMV` argument. This argument is only considered when`'reset` is set to `FALSE`. When `mcmc$run(niter, reset = FALSE, resetMV = TRUE)` is called the internal modelValues objects `mvSamples` and `mvSamples2` are each resized to the appropriate length for holding the requested number of samples (`niter/thin`, and `niter/thin2`, respectively) and the MCMC carries on from where it left off. In other words, the previously obtained samples are deleted (e.g. to reduce memory usage) prior to continuing the MCMC. The default value of  `resetMV`  is  `FALSE`. 




### Measuring sampler computation times: *getTimes* {#sec:sampler-time}

If you want to obtain the computation time spent in each sampler, you can set `time=TRUE` as a run-time argument and then use the method `getTimes()` obtain the times.  For example,

```{r, eval=FALSE}
Cmcmc$run(niter, time = TRUE)
Cmcmc$getTimes()
``` 
will return a vector of the total time spent in each sampler, measured in seconds. 
<!---  
%### Modifying the order of sampler execution: `samplerExecutionOrder` \label{sec:runtime-sampler-execution-order}
%The order in which the MCMC sampler functions execute can also be specified at MCMC runtime.  This is done using the argument `samplerExecutionOrder`.  Providing this runtime argument will override any modified execution ordering that was specified in the MCMC configuration.  For example,
% code chunk example (with eval=FALSE):
## interleave execution of the first sampler with other sampler functions 
% Cmcmc$run(niter, samplerExecutionOrder = c(1, 2, 1, 3, 1, 4, 1, 5))
-->

### Assessing the adaption process of *RW* and *RW_block* samplers

If you'd like to see the evolution (over the iterations) of the acceptance proportion and proposal scale information, you can use some internal methods provided by NIMBLE, after setting two options to make the history accessible. Here we assume that `cMCMC` is the compiled MCMC object and `idx` is the numeric index of the sampler function you want to access from amongst the list of sampler functions that are part of the MCMC.

```{r, eval=FALSE}
## set options to make history accessible
nimbleOptions(buildInterfacesForCompiledNestedNimbleFunctions = TRUE)
nimbleOptions(MCMCsaveHistory = TRUE)
## Next, set up and run your MCMC.
## Now access the history information:
Cmcmc$samplerFunctions[[idx]]$getScaleHistory()
Cmcmc$samplerFunctions[[idx]]$getAcceptanceHistory()
Cmcmc$samplerFunctions[[idx]]$getPropCovHistory()   ## only for RW_block
```

Note that modifying elements of the control list may greatly
     affect the performance of the `RW_block` sampler. In particular, the sampler
     can take a long time to find a good proposal covariance when the
     elements being sampled are not on the same scale. We recommend
     providing an informed value for ‘propCov’ in this case (possibly
     simply a diagonal matrix that approximates the relative scales),
     as well as possibly providing a value of ‘scale’ that errs on the
     side of being too small. You may also consider decreasing
     ‘adaptFactorExponent’ and/or ‘adaptInterval’, as doing so has
     greatly improved performance in some cases.

## Extracting MCMC samples {#sec:extracting-samples}

After executing the MCMC, the output samples can be extracted as follows:

```{r, samples, eval=FALSE}
mvSamples <- mcmc$mvSamples
mvSamples2 <- mcmc$mvSamples2
```

These *modelValues* objects can be converted into matrices using `as.matrix` or lists using `as.list`:

```{r, as.matrix-samples, eval=FALSE}
samplesMatrix <- as.matrix(mvSamples)
samplesList <- as.list(mvSamples)
samplesMatrix2 <- as.matrix(mvSamples2)
samplesList2 <- as.list(mvSamples2)
```

The column names of the matrices will be the node names of nodes in the monitored variables.  Then, for example, the mean of the samples for node `x[2]` could be calculated as:

```{r, access-samples, eval=FALSE}
mean(samplesMatrix[, "x[2]"])
```

The list version will contain an element for each variable that will be the size and shape of the variable with an additional index for MCMC iteration.  By default MCMC iteration will be the first index, but including `iterationAsLastIndex = TRUE` will make it the last index.

Obtaining samples as matrices or lists is most common, but see Section \@ref(sec:modelValues-struct) for more about programming with modelValues objects, especially if you want to write nimbleFunctions to use the samples.

## Calculating WAIC {#sec:WAIC}

The WAIC [@Watanabe_2010] can be calculated from the posterior samples produced during the MCMC algorithm. Users have two options to calculate WAIC. The main approach requires enabling WAIC when setting up the MCMC and allows access to a variety of ways to calculate WAIC. This approach does not require that any specific monitors be set^[Prior to version 0.12.0, NIMBLE required specific monitors, because WAIC was calculated at the end of the MCMC using the entire set of MCMC samples] because the WAIC is calculated in an online manner, accumulating the necessary quantities to calculate WAIC as the MCMC is run.

The second approach allows one to calculate the WAIC after an MCMC has been run using an MCMC object or matrix (or dataframe) containing the posterior samples, but it requires the user to have correctly specified the monitored variables and only provides the default way to calculate WAIC.

Here we first discuss the main approach and then close the section by showing the second approach.

To enable WAIC for the first approach, the argument `enableWAIC = TRUE` must be supplied to `configureMCMC` (or to `buildMCMC` if not using `configureWAIC`, or the `enableWAIC` NIMBLE option must have been set to `TRUE`.

The WAIC (as well as the pWAIC and lppd values) is extracted by the member method `mcmc$getWAIC` (see `help(waic)` in R for more details) or is available as the `WAIC` element of the `runMCMC` or `nimbleMCMC` output lists. One can use the member method `mcmc$getWAICdetails` for additional quantities related to WAIC that are discussed below.


```{r, eval=FALSE}
Cmcmc$getWAIC()
Cmcmc$getWAICdetails()
```

Note that there is not a unique value of WAIC for a model. WAIC is calculated from
Equations 5, 12, and 13 in [@Gelman_etal_2014] (i.e., using \emph{p}WAIC2), as discussed in detail in [@Hug_Paciorek_2021]. Therefore, NIMBLE provides
user control over how WAIC is calculated in two ways.

First, by default NIMBLE provides the conditional WAIC, namely the version of WAIC where all parameters directly involved in the likelihood are treated as $\theta$ for the purposes of Equation 5 from [@Gelman_etal_2014]. Users can request the marginal WAIC (see [@Ariyo_etal_2019]), namely the version of WAIC where latent variables are integrated over (i.e., using a marginal likelihood). This is done by providing the `waicControl` list with a `marginalizeNodes` element to `configureMCMC` or `buildMCMC` (when providing a model as the argument to `buildMCMC`). See `help(waic)` for more details.

Second, WAIC relies on a partition of the observations, i.e., 'pointwise' prediction. By default, in NIMBLE the sum over log pointwise predictive density values treats each data node as contributing a single value to the sum. When a data node is multivariate, that data node contributes a single value to the sum based on the joint density of the elements in the node. If one wants to group data nodes such that the joint density within each group is used, one can provide the `waicControl` list with a `dataGroups` element to `configureMCMC` or `buildMCMC` (when providing a model as the argument to `buildMCMC`). See `help(waic)` for more details.

Marginal WAIC requires using Monte Carlo simulation at each iteration of the MCMC to average over draws for the latent variables. To assess the stability of the marginal WAIC to the number of Monte Carlo iterations, one can examine how the WAIC changes with increasing iterations (up to the full number of iterations specified via the `nItsMarginal` element of the `waicControl` list) based on the `WAIC_partialMC`, `lppd_partialMC`, and `pWAIC_partialMC` elements of the detailed WAIC output.

For comparing WAIC between two models, [@Vehtari_etal_2017] discuss using the per-observation (more generally, per-data group) contributions to the overall WAIC values to get an approximate standard error for the difference in WAIC between the models. These element-wise values are available as the `WAIC_elements`, `lppd_elements`, and `pWAIC_elements` components of the detailed WAIC output.

The second overall approach to WAIC available in NIMBLE allows one to calculate WAIC post hoc after MCMC sampling using an MCMC object or matrix (or dataframe) containing posterior samples. Simply set up the MCMC and run it without enabling WAIC (but making sure to include in the monitors all stochastic parent nodes of the data nodes) and then use the function `calculateWAIC`, as shown in this example:

```{r, eval=FALSE}
samples <- runMCMC(Cmcmc, niter = 10000, nburnin = 1000)
## Using calculateWAIC with an MCMC object
calculateWAIC(Cmcmc)
## Using calculateWAIC with a matrix of samples
calculateWAIC(samples, model)
```

This approach only provides conditional WAIC without any grouping of data nodes (though one can achieve grouping by grouping data nodes into multivariate nodes).

## k-fold cross-validation

The `runCrossValidate` function in NIMBLE performs k-fold cross-validation on a `nimbleModel` fit via MCMC.  More information can be found by calling `help(runCrossValidate)`. 

## Variable selection using Reversible Jump MCMC {#sec:rjmcmc}

A common method for Bayesian variable selection in regression-style problems is to define a space of different models and have MCMC sample over the models as well as the parameters for each model.  The models differ in which explanatory variables are included.  Often this idea is implemented in the BUGS language by writing the largest possible model and including indicator variables to turn regression parameters off and on (see model code below for an example of how indicator variables can be used).  Foramally, that approach doesn't sample between different models, instead embedding the models in one large model.  However, that approach can result in poor mixing and require compromises in choices of prior distributions because a given regression parameter will sample from its prior when the corresponding indicator is 0.  It is also computationally wasteful, because sampling effort will be spent on a coefficient even if it has no effect because the corresponding indicator is 0.

A different view of the problem is that the different combinations of coefficients represent models of different dimensions.  Reversible Jump MCMC [@Green_1995] (RJMCMC) is a general framework for MCMC simulation in which the dimension of the parameter space can vary between iterates of the Markov chain. The reversible jump sampler can be viewed as an extension of the Metropolis-Hastings algorithm onto more general state spaces.  NIMBLE provides an implementation of RJMCMC for variable selection that requires the user to write the largest model of interest and supports use of indicator variables but formally uses RJMCMC sampling for better mixing and efficiency.  When a coefficient is not in the model (or its indicator is 0), it will not be sampled, and it will therefore not follow its prior in that case.

In technical detail, given two models $M_1$ and $M_2$ of possibly different dimensions, the core idea of RJMCMC is to remove the difference in the dimensions of models $M_1$ and $M_2$ by supplementing the corresponding parameters $\boldsymbol{\theta_1}$ and $\boldsymbol{\theta_2}$ with auxiliary variables $\boldsymbol{u}_{1 \rightarrow 2}$ and $\boldsymbol{u}_{2 \rightarrow 1}$ such that $(\boldsymbol{\theta_1}, \boldsymbol{u}_{1 \rightarrow 2})$ and $(\boldsymbol{\theta_2}, \boldsymbol{u}_{2 \rightarrow 1})$
are in bijection, $(\boldsymbol{\theta_2}, \boldsymbol{u}_{2 \rightarrow 1}) = \Psi(\boldsymbol{\theta_1}, \boldsymbol{u}_{1 \rightarrow 2})$. The corresponding Metropolis-Hastings acceptance probability is generalized accounting for the proposal density for the auxiliary variables.  

NIMBLE implements RJMCMC for variable selection using a univariate normal distribution centered on 0 (or some fixed value) as the proposal density for parameters being added to the model.  Two ways to set up models for RJMCMC are supported, which differ by whether the inclusion probabilities for each parameter are assumed known or must be written in the model:

  - If the inclusion probabilities are assumed known, then RJMCMC may be used with regular model code, i.e. model code written without heed to variable selection.
  - If the inclusion probability is a parameter in the model, perhaps with its own prior, then RJMCMC requires that indicator variables be written in the model code.  The indicator variables will be sampled using RJMCMC, but otherwise they are like any other nodes in the model.
  
The steps to set up variable selection using RJCMCM are:

  1. Write the model with indicator variables if needed.
  2. Configure the MCMC as usual with `configureMCMC`.
  3. Modify the resulting MCMC configuration object with `configureRJ`.

The `configureRJ` function modifies the MCMC configuration to (1) assign samplers that turn on and off variable in the model and (2) modify the existing samplers for the regression coefficients to use 'toggled' versions.  The toggled versions invoke the samplers only when corresponding variable is currently in the model. In the case where indicator variables are included in the model, sampling to turn on and off variables is done using `RJ_indicator` samplers. In the case where indicator variables are not included, sampling is done using `RJ_fixed_prior` samplers. 

<!--  See `help(configureRJ)` for details. -->

In the following we provide an example for the two different model specifications.

### Using indicator variables {#sec:rjmcmc-indicator}

Here we consider a normal linear regression in which two covariates `x1` and `x2` are candidates to be included in the model.  The two corresponding coefficients are `beta1` and `beta2`.  The indicator variables are `z1` and `z2`, and their inclusion probability is `psi`.  As described below, one can also use vectors for a set of coefficients and corresponding indicator variables.

```{r indicators, results = "hide"}
## Linear regression with intercept and two covariates
code <- nimbleCode({
  beta0 ~ dnorm(0, sd = 100)
  beta1 ~ dnorm(0, sd = 100)
  beta2 ~ dnorm(0, sd = 100)
  sigma ~ dunif(0, 100) 
  z1 ~ dbern(psi)  ## indicator variable associated with beta1
  z2 ~ dbern(psi)  ## indicator variable associated with beta2
  psi ~ dbeta(1, 1) ## hyperprior on inclusion probability
  for(i in 1:N) {
    Ypred[i] <- beta0 + beta1 * z1 * x1[i] + beta2 * z2 * x2[i]
    Y[i] ~ dnorm(Ypred[i], sd = sigma)
  }
})

## simulate some data
set.seed(1)
N <- 100
x1 <- runif(N, -1, 1)
x2 <- runif(N, -1, 1) ## this covariate is not included
Y <- rnorm(N, 1.5 + 2 * x1, sd = 1)


## build the model and configure default MCMC

RJexampleModel <- nimbleModel(code, constants = list(N = N),
                              data = list(Y = Y, x1 = x1, x2 = x2), 
                              inits = list(beta0 = 0, beta1 = 0, beta2 = 0,
                              sigma = sd(Y), z2 = 1, z1 = 1, psi = 0.5))

RJexampleConf <- configureMCMC(RJexampleModel)
```

```{r print samplers}
## For illustration, look at the default sampler assignments
RJexampleConf$printSamplers()
```

At this point we can modify the MCMC configuration object, `RJexampleConf`, to use reversible jump samplers for selection on `beta1` and `beta2`.


```{r indicators: configureRJ, results = "hide"}
configureRJ(conf = RJexampleConf,     
            targetNodes = c("beta1", "beta2"),
            indicatorNodes = c('z1', 'z2'),
            control = list(mean = c(0, 0), scale = 2))
```

The `targetNodes` argument gives the coefficients (nodes) for which we want to do variable selection.  The `indicatorNodes` gives the corresponding indicator nodes, ordered to match `targetNodes`.  The `control` list gives the means and scale (standard deviation) for normal reversible jump proposals for `targetNodes`.  The means will typically be 0  (by default `mean = 0` and `scale = 1`), but they could be anything.   An optional `control` element called `fixedValue` can be provided in the non-indicator setting; this gives the value taken by nodes in `targetNodes` when they are out of the model (by default, `fixedValue` is 0). 
All  `control` elements can be scalars or vectors.  A scalar values will be used for all `targetNodes`.  A vector value must be of equal length as `targetNodes` and will be used in order.

To use RJMCMC on a vector of coefficients with a corresponding vector of indicator variables, simply provide the variable names for `targetNodes` and `indicatorNodes`.  For example, `targetNodes = "beta"` is equivalent to `targetNodes = c("beta[1]", "beta[2]")` and `indicatorNodes = "z"` is equivalent to `indicatorNodes = c("z[1]", "z[2]")`.   Expansion of variable names into a vector of node names will occur  as described in Section \@ref(sec:arbitr-coll-nodes).   When using this method, *both* arguments must be provided as variable names to be expanded.

Next we can see the result of `configureRJ` by printing the modified list of samplers:
```{r indicators: print samplers after configureRJ}
RJexampleConf$printSamplers()
```

An `RJ_indicator` sampler was assigned to each of `z[1]` and `z[2]` in place of the `binary` sampler, while the samplers for `beta[1]`and `beta[2]` have been changed to  `RJ_toggled` samplers.  The latter samplers contain the original samplers, in this case `conjugate_dnorm_dnorm` samplers, and use them only when the corresponding indicator variable is equal to $1$ (i.e., when the coefficient is in the model). 

Notice that the order of the samplers has changed, since `configureRJ` calls `removeSampler` for nodes in `targetNodes` and `indicatorNodes`, and subsequently `addSampler`, which appends the sampler to the end of current sampler list. Order can be modified by using `setSamplers`.

Also note that `configureRJ` modifies the MCMC configuration (first argument) and returns `NULL`.

### Without indicator variables {#sec:rjmcmc-no-indicator}

We consider the same regression setting, but without the use of indicator variables and with fixed probabilities of including each coefficient in the model.

```{r no_indicators, results = "hide"}

## Linear regression with intercept and two covariates
code <- nimbleCode({
  beta0 ~ dnorm(0, sd = 100)
  beta1 ~ dnorm(0, sd = 100)
  beta2 ~ dnorm(0, sd = 100)
  sigma ~ dunif(0, 100)
  for(i in 1:N) {
    Ypred[i] <- beta0 + beta1 * x1[i] + beta2 * x2[i]
    Y[i] ~ dnorm(Ypred[i], sd = sigma)
  }
})

## build the model
RJexampleModel2 <- nimbleModel(code, constants = list(N = N),
                               data = list(Y = Y, x1 = x1, x2 = x2), 
                               inits = list(beta0 = 0, beta1 = 0, beta2 = 0,
                                            sigma = sd(Y)))

RJexampleConf2 <- configureMCMC(RJexampleModel2)
```

```{r no indicators: print samplers}
## print NIMBLE default samplers
RJexampleConf2$printSamplers()
```

In this case, since there are no indicator variables, we need to pass to `configureRJ` the prior inclusion probabilities for each node in `targetNodes`, by specifying either one common value or a vector of values for the argument `priorProb`. This case does not allow for a stochastic prior. 


```{r no indicators: configureRJ, results = "hide"}
configureRJ(conf = RJexampleConf2,     
            targetNodes = c("beta1", "beta2"),
            priorProb = 0.5,
            control = list(mean = 0, scale = 2, fixedValue = c(1.5, 0)))
```

```{r no indicators: print samplers after configureRJ}
## print samplers after configureRJ
RJexampleConf2$printSamplers()
```

Since there are no indicator variables, the `RJ_fixed_prior` sampler is assigned directly to each of `beta[1]` and `beta[2]` along with the `RJ_toggled` sampler.  The former sets a coefficient to its `fixedValue` when it is out of the model.  The latter invokes the  regular sampler for the coefficient  only if it is in the model at a given iteration. 

If `fixedValue` is given when using `indicatorNodes` the values provided in `fixedValue` are ignored.  However the same behavior can be obtained in this situation, using a different model specification. For example, the model in \@ref(sec:rjmcmc-indicator) can be modified to have `beta1` equal to $1.5$ when not in the model as follows:

```{r indicators v2, results = "hide", eval = FALSE}
  for(i in 1:N) {
    Ypred[i] <- beta0 + (1 - z1) * 1.5 * beta1 * x1[i] +
                z1 * beta1 * x1[i] + beta2 * z2 * x2[i]
    Y[i] ~ dnorm(Ypred[i], sd = sigma)
  }
```

## Samplers provided with NIMBLE {#sec:samplers-provided}

Most documentation of MCMC samplers provided with NIMBLE can be found by invoking `help(samplers)` in R.  Here we provide additional explanation of conjugate samplers and how complete customization can be achieved by making a sampler use an arbitrary log-likelihood function, such as to build a particle MCMC algorithm.



### Conjugate ('Gibbs') samplers

By default, `configureMCMC()` and `buildMCMC()` will assign conjugate samplers to all nodes satisfying a conjugate relationship, unless the option `useConjugacy = FALSE` is specified.  

The current release of NIMBLE supports conjugate sampling of the relationships listed in Table \@ref(tab:conjugacy)^[NIMBLE's internal definitions of these relationships can be viewed with `nimble:::conjugacyRelationshipsInputList`.].

```{r, child = 'tables/conjugacyTable.md'}
```

Conjugate sampler functions may (optionally) dynamically check that their own posterior likelihood calculations are correct.  If incorrect, a warning is issued.  However, this functionality will roughly double the run-time required for conjugate sampling.  By default, this option is disabled in NIMBLE.  This option may be enabled with
`nimbleOptions(verifyConjugatePosteriors = TRUE)`.

If one wants information about conjugate node relationships for other purposes, they can be obtained using the `checkConjugacy` method on a model.  This returns a named list describing all conjugate relationships.  The `checkConjugacy` method can also accept a character vector argument specifying a subset of node names to check for conjugacy. 
<!---  ### `binary (Gibbs) sampler` -->

<!---  The `binary` sampler performs Gibbs sampling for binary-valued nodes (strictly taking values 0 or 1).  This can only be used for nodes following either a `dbern(p)` or `dbinom(p, size=1)` distribution.  The `binary` sampler accepts no control list arguments. -->

<!---  ### `RW_multinomial sampler` -->

<!---  The `RW_multinomial` sampler operates exclusively on nodes following a multinomial distribution.  The sampler performs a series of Metropolis-Hastings steps between pairs of groups.  Proposals are generated via a draw from a binomial distribution, whereafter the proposed number density is moved from one group to another group.  The acceptance or rejection of these proposals follows a standard Metropolis-Hastings procedure.  Probabilities for the random binomial proposals are adapted to a target acceptance rate of 0.5. -->

<!---  The `RW_multinomial` sampler can be customized using `control` list arguments to set the adaptive properties of the sampler.  See `help(samplers)` for details.  -->
 
<!---  ### `RW_dirichlet sampler` -->

<!---  The `RW_dirichlet` sampler operates exclusively on nodes following a Dirichlet distribution, and is designed for the non-conjugate Dirichlet case.  The sampler performs independent adaptive Metropolis-Hastings updates on a reparameterization of the target distribution, expressed in terms of component univariate gamma distributions. -->

<!---  The `RW_dirichlet` sampler can be customized using `control` list arguments to set the adaptive properties of the sampler.  See `help(samplers)` for details. -->

<!---  ### Scalar Metropolis-Hastings random walk `RW sampler` -->

<!---  The `RW` sampler executes adaptive Metropolis-Hastings sampling with a normal proposal distribution, implementing the adaptation routine given in [@Shaby2011].  This sampler can be applied to any scalar continuous-valued stochastic node, and can optionally sample on a log scale. -->

<!---  The `RW` sampler can be customized using the `control` list argument to set the initial proposal distribution scale, the adaptive properties of the sampler, the reflective property of proposals, and whether to sample on a log scale.  See `help(samplers)` for details.  -->
 
<!---  Note that because we use a simple normal proposal distribution on all nodes, negative proposals may be simulated for non-negative random variables. These will be rejected, so the only downsides to this are some inefficiency and the presence of warnings during uncompiled (but not compiled) execution indicating `NA` or `NaN` values.  This can be avoided in some cases by using the option `reflective = TRUE`, which reflects proposal values to stay within the range of the target node distribution. -->

<!---  ### Multivariate Metropolis-Hastings `RW_block sampler` -->

<!---  The `RW_block` sampler performs a simultaneous update of one or more model nodes, using an adaptive Metropolis-Hastings algorithm with a multivariate normal proposal distribution [@Roberts_Sahu_1997], implementing the adaptation routine given in [@Shaby2011].  This sampler may be applied to any set of continuous-valued model nodes, to any single continuous-valued multivariate model node, or to any combination thereof.  -->

<!---  The `RW_block` sampler can be customized using the `control` list argument to set the initial proposal covariance, and the adaptive properties of the sampler.  See `help(samplers)` for details.  -->

<!---  ### `slice sampler` -->

<!---  The `slice` sampler performs slice sampling of the scalar node to which it is applied [@Neal2003].  This sampler can operate on either continuous-valued or discrete-valued scalar nodes.  The slice sampler performs a 'stepping out' procedure, in which the slice is iteratively expanded to the left or right by an amount `sliceWidth`.  This sampler is optionally adaptive, whereby the value of `sliceWidth` is adapted towards the observed absolute difference between successive samples. -->

<!---  The `slice` sampler can be customized using the `control` list argument to set the initial slice width, and the adaptive properties of the sampler.  See `help(samplers)` for details.  -->

<!---  ### Elliptical slice sampling: `ess sampler` -->

<!---  The `ess` sampler performs elliptical slice sampling of a single node, which must follow a multivariate normal distribution [@2010arXiv1001.0175M].  The algorithm is an extension of slice sampling [@Neal2003], generalized to the multivariate normal context.  An auxiliary variable is used to identify points on an ellipse (which passes through the current node value) as candidate samples, which are accepted contingent upon a likelihood evaluation at that point.  This algorithm requires no tuning parameters and therefore no period of adaptation, and may result in very efficient sampling from multivariate Gaussian distributions. -->

<!---  The `ess` sampler accepts no control list arguments.  See `help(samplers)` for details. -->

<!---  ### Hierarchical `crossLevel sampler` -->

<!---  This sampler is constructed to perform simultaneous updates across two levels of stochastic dependence in the model structure.  This is possible when all stochastic descendants of node(s) at one level have conjugate relationships with their own stochastic descendants.  In this situation, a Metropolis-Hastings algorithm may be used, in which a multivariate normal proposal distribution is used for the higher-level nodes, and the corresponding proposals for the lower-level nodes undergo Gibbs (conjugate) sampling.  The joint proposal is either accepted or rejected for all nodes involved based upon the Metropolis-Hastings ratio. -->

<!---  The `crossLevel` sampler can be customized using the `control` list argument to set the initial proposal covariance and the adaptive properties for the Metropolis-Hastings sampling of the higher-level nodes.  See `help(samplers)` for details.  -->

<!---  The requirement that all stochastic descendants of the `target` nodes must themselves have only conjugate descendants will be checked when the MCMC algorithm is built.  This sampler is useful when there is strong dependence across the levels of a model that causes problems with convergence or mixing. -->

### Customized log-likelihood evaluations: *RW_llFunction sampler*
 
Sometimes it is useful to control the log-likelihood calculations used for an MCMC updater instead of simply using the model.  For example, one could use a sampler with a log-likelihood that analytically (or numerically) integrates over latent model nodes.  Or one could use a sampler with a log-likelihood that comes from a stochastic approximation such as a particle filter (see below), allowing composition of a particle MCMC (PMCMC) algorithm [@Andrieu_Doucet_Holenstein_2010].  The `RW_llFunction` sampler handles this by using a Metropolis-Hastings algorithm with a normal proposal distribution and a user-provided log-likelihood function.  To allow compiled execution, the log-likelihood function must be provided as a specialized instance of a nimbleFunction.  The log-likelihood function may use the same model as the MCMC as a setup argument (as does the example below), but if so the state of the model should be unchanged during execution of the function (or you must understand the implications otherwise).

The `RW_llFunction` sampler can be customized using the `control` list argument to set the initial proposal distribution scale and the adaptive properties for the Metropolis-Hastings sampling.  In addition, the `control` list argument must contain a named `llFunction` element. This is the specialized nimbleFunction that calculates the log-likelihood; it must accept no arguments and return a scalar double number.  The return value must be the total log-likelihood of all stochastic dependents of the `target` nodes -- and, if `includesTarget = TRUE`, of the target node(s) themselves --  or whatever surrogate is being used for the total log-likelihood.  This is a required `control` list element with no default.  See `help(samplers)` for details. 

Here is a complete example:

```{r, RW_ll, eval=FALSE}
code <- nimbleCode({
    p ~ dunif(0, 1)
    y ~ dbin(p, n)
})

Rmodel <- nimbleModel(code, data = list(y=3), inits = list(p=0.5, n=10))

llFun <- nimbleFunction(
    setup = function(model) { },
    run = function() {
        y <- model$y
        p <- model$p
        n <- model$n
        ll <- lfactorial(n) - lfactorial(y) - lfactorial(n-y) +
              y * log(p) + (n-y) * log(1-p)
        returnType(double())
        return(ll[1])
    }
)

RllFun <- llFun(Rmodel)

mcmcConf <- configureMCMC(Rmodel, nodes = NULL)

mcmcConf$addSampler(target = "p", type = "RW_llFunction",
    control = list(llFunction = RllFun, includesTarget = FALSE))

Rmcmc <- buildMCMC(mcmcConf)
```

Note that we need to return `ll[1]` and not just `ll` because there are no scalar variables in compiled models. Hence `y` and other variables, and therefore `ll`, are of dimension 1 (i.e., vectors / one-dimensional arrays), so we need to specify the first element in order to have the return type be a scalar.

<!---  ### Terminal node `posterior_predictive sampler` 
% The `posterior_predictive` sampler is only appropriate for use on terminal stochastic nodes (that is, those having no stochastic dependencies).  Note that such nodes play no role in inference but have often been included in BUGS models to accomplish posterior predictive checks.  NIMBLE allows posterior predictive values to be simulated independently of running MCMC, for example by writing a nimbleFunction to do so.  This means that in many cases where terminal stochastic nodes have been included in BUGS models, they are not needed when using NIMBLE.
The `posterior_predictive` sampler functions by calling the `simulate` method of the relevant node, then updating model probabilities and deterministic dependent nodes.  The `posterior_predictive` sampler will automatically be assigned to all terminal, non-data stochastic nodes in a model by the default MCMC configuration, so it is uncommon to manually assign this sampler.  The `posterior_predictive` sampler accepts no control list arguments. -->


### Particle MCMC sampler
For state space models, a particle MCMC (PMCMC) sampler can be specified for top-level parameters.  This sampler is described in Section \@ref(sec:particle-mcmc).

## Detailed MCMC example: *litters* {#sec:mcmc-example-litters}

Here is a detailed example of specifying, building, compiling, and running two MCMC algorithms.  We use the `litters` example from the BUGS examples.

```{r, MCMC-litters, eval=FALSE}

###############################
##### model configuration #####
###############################

# define our model using BUGS syntax
litters_code <- nimbleCode({
    for (i in 1:G) {
        a[i] ~ dgamma(1, .001)
        b[i] ~ dgamma(1, .001)
        for (j in 1:N) {
            r[i,j] ~ dbin(p[i,j], n[i,j])
            p[i,j] ~ dbeta(a[i], b[i]) 
        }
        mu[i] <- a[i] / (a[i] + b[i])
        theta[i] <- 1 / (a[i] + b[i])
    }
})

# list of fixed constants
constants <- list(G = 2,
                  N = 16,
                  n = matrix(c(13, 12, 12, 11, 9, 10, 9,  9, 8, 11, 8, 10, 13,
                      10, 12, 9, 10,  9, 10, 5,  9,  9, 13, 7, 5, 10, 7,  6, 
                      10, 10, 10, 7), nrow = 2))

# list specifying model data
data <- list(r = matrix(c(13, 12, 12, 11, 9, 10,  9, 9, 8, 10, 8, 9, 12, 9,
                 11, 8, 9,  8,  9,  4, 8,  7, 11, 4, 4, 5 , 5, 3,  7, 3, 
                 7, 0), nrow = 2))

# list specifying initial values
inits <- list(a = c(1, 1),
              b = c(1, 1),
              p = matrix(0.5, nrow = 2, ncol = 16),
              mu    = c(.5, .5),
              theta = c(.5, .5))

# build the R model object
Rmodel <- nimbleModel(litters_code,
                      constants = constants,
                      data      = data,
                      inits     = inits)


###########################################
##### MCMC configuration and building #####
###########################################

# generate the default MCMC configuration;
# only wish to monitor the derived quantity "mu"
mcmcConf <- configureMCMC(Rmodel, monitors = "mu")

# check the samplers assigned by default MCMC configuration
mcmcConf$printSamplers()

# double-check our monitors, and thinning interval
mcmcConf$printMonitors()

# build the executable R MCMC function
mcmc <- buildMCMC(mcmcConf)

# let's try another MCMC, as well,
# this time using the crossLevel sampler for top-level nodes

# generate an empty MCMC configuration
# we need a new copy of the model to avoid compilation errors
Rmodel2 <- Rmodel$newModel()
mcmcConf_CL <- configureMCMC(Rmodel2, nodes = NULL, monitors = "mu")

# add two crossLevel samplers
mcmcConf_CL$addSampler(target = c("a[1]", "b[1]"), type = "crossLevel")
mcmcConf_CL$addSampler(target = c("a[2]", "b[2]"), type = "crossLevel")

# let's check the samplers
mcmcConf_CL$printSamplers()

# build this second executable R MCMC function
mcmc_CL <- buildMCMC(mcmcConf_CL)


###################################
##### compile to C++, and run #####
###################################

# compile the two copies of the model
Cmodel <- compileNimble(Rmodel)
Cmodel2 <- compileNimble(Rmodel2)

# compile both MCMC algorithms, in the same
# project as the R model object
# NOTE: at this time, we recommend compiling ALL
# executable MCMC functions together
Cmcmc <- compileNimble(mcmc, project = Rmodel)
Cmcmc_CL <- compileNimble(mcmc_CL, project = Rmodel2)

# run the default MCMC function,
# and example the mean of mu[1]
Cmcmc$run(1000)
cSamplesMatrix <- as.matrix(Cmcmc$mvSamples) # alternative: as.list
mean(cSamplesMatrix[, "mu[1]"])

# run the crossLevel MCMC function,
# and examine the mean of mu[1]
Cmcmc_CL$run(1000)
cSamplesMatrix_CL <- as.matrix(Cmcmc_CL$mvSamples)
mean(cSamplesMatrix_CL[, "mu[1]"])


###################################
#### run multiple MCMC chains #####
###################################

# run 3 chains of the crossLevel MCMC
samplesList <- runMCMC(Cmcmc_CL, niter=1000, nchains=3)

lapply(samplesList, dim)
```

## Comparing different MCMCs with *MCMCsuite* and *compareMCMCs* {#mcmc-suite-compare-mcmcs}

Please see the `compareMCMCs` package for the features previously provided by `MCMCsuite` and `compareMCMCs` in NIMBLE (until version 0.8.0).  The `compareMCMCs` package provides tools to automatically run MCMC in nimble (including multiple sampler configurations), WinBUGS, OpenBUGS, JAGS, Stan, or any other engine for which you provide a simple common interface.  The package makes it easy to manage comparison metrics and generate html pages with comparison figures.

## Running MCMC chains in parallel

It is possible to run multiple chains in parallel using standard R parallelization packages such as `parallel`, `foreach`, and `future`. However, you must create separate copies of all model and MCMC objects using `nimbleModel`, `buildMCMC`, `compileNimble`, etc. This is because NIMBLE uses Reference Classes and R6 classes, so copying such objects simply creates a new variable name that refers to the original object.

Thus, in your parallel loop or lapply-style statement, you should run `nimbleModel` and all subsequent calls to create and compile the model and MCMC algorithm within the parallelized block of code, once for each MCMC chain being run in parallel.

For a worked example, please see [the parallelization example on our webpage](https://r-nimble.org/nimbleExamples/parallelizing_NIMBLE.html).


